1.	基本操作:列表,元组,字典,数组,序列之间的异同点
数据结构：通过编号组合起来的数据元素（数、字符或其他数据结构）集合。
序列是python中最基本的数据结构。序列中的成员是有序排列的，可以通过下标偏移量访问到它的一个或多个成员，序列包括列表和元组。所有的序列都可以进行某些特定的操作：索引、切片、加、乘、成员资格检查、计算序列长度、找出最大最小元素等。
索引：序列中所有的元素都有编号，索引从0开始递增。使用负数索引时，python会从右边开始计数（-1指的是序列最右边的元素）
切片：除了可以使用索引访问单个元素，还可以使用切片来访问一定范围内的元素。切片操作需要两个索引来作为边界，第一个索引的元素是包含在切片内的，而第二个则不包含。切片中隐含一个参数—步长，默认值为1，步长不能为0，但是可以为负数（从右向左提取元素）。使用一个负数作为步长时，必须让开始点大于结束点。
加：使用加号可以对两个类型相同的序列进行连接操作
乘：使用数字n乘以一个序列会生成新的序列，在新的序列中，原来的序列将被重复n次
成员资格：可以使用in或not in运算符判断一个对象是否为某个序列的成员
序列长度、最大最小元素：利用python的内置函数len/min/max

列表（list）：用[]定义，列表是可以修改的（给元素赋值、删除元素、给切片赋值以及使用列表的方法（append、clear、copy、count、extend、index、insert、pop、remove、reverse、sort、高级排序））
元组（tuple）：用()定义，元组是不可变序列，元组中的数据确立就不能改变，不能对元组中的元素进行增删改操作，因此元组没有增加元素append、修改元素、删除元素pop的相关方法，只能通过索引访问元组中的成员,元组中的成员的起始序号为0。只要将一些值用逗号分隔或者用圆括号括起，就能自动创建一个元组，就算元组中只有一个值后面也需要加逗号。对元组的访问方式和其他序列相同。元组中可以包含任何数据类型，也可以包含另一个元组。
字典：数据结构类型为映射，它是由键（数、字符串或元组，且唯一）及其相应的值组成，键—值对称为项。每个键与其值之间用冒号分隔，各项之间用逗号分隔，整个字典放在花括号内。字典的基本行为在很多方面都类似于序列，但也有一些不同之处。如在添加元素上面，字典可以为其中没有的键赋值，而序列如果不使用append或其他类似的方法，就不能给列表中没有的元素赋值；如在检查成员资格时表达式k in d（d是一个字典）查找的是键而不是值，而表达式v in l（l是个列表）查找的是值而不是索引。字典也有方法（clear、copy、fromkeys、get、items、keys、pop、popitem、setdefault、update、values）
数组：Python 没有内置对数组的支持，但可以使用 Python 列表代替。python中的Numpy库也可以实现对数组的定义，Numpy提供了一个N维数组类型ndarray，它描述了相同类型的元素的集合，它与列表的区别主要有：1.ndarray数据与数据的地址都是连续的，而列表存储的是引用地址；2.ndarray中的所有元素的类型都是相同的，而列表中的元素类型是任意的；3.ndarray内置了并行运算功能，当系统有多个核心时，Numpy会自动做并行运算。
集合：集合是一个无序不重复元素序列，可以使用大括号{}或者set()函数创建集合，创建一个空集合必须用 set( ) 而不是 { }，因为 { } 是用来创建一个空字典的。集合不支持索引和切片，只支持成员操作符和for循环遍历。


2.	创建一维数组的方式，它们之间的差别
1.定义法
直接定义：matrix=[0,1,2,3]   间接定义：matrix=[0 for i in range(4)]
2.Numpy方法
Numpy内置了从头开始创建数组的函数：zeros(shape)将创建一个用指定形状用0填充的数组，np.array()，np.arrange()，np.linspace()等。
3.其他转换法
数组还有比较常用的一种方法，就是从其他Python结构（列表，元组）转换
列表转数组：                        元组转数组                 
  
4.TensorFlow
TensorFlow 的名称源自张量，张量是任意维度的数组，TensorFlow 指令会创建、销毁和操控张量。用TensorFlow生成一维数组的函数与numpy的方法相同，同样需要注意的是，数组是从0开始的，而不是从1开始的。但是与Numpy不同的地方是，TensorFlow生成的一维数组无法进行迭代运算。














3.	python中基本控制语句:循环,判断,写简单程序
Python有两个原始的循环命令：while循环和for循环。
While循环：如果使用while循环，只要条件为真，就可以执行一组语句。
打印小于7的数，while循环需要准备好相关的变量，需要定义一个索引变量i令其值为1，同时需要递增i，否则会无限循环下去。如果使用break语句，即使while条件为真，也可以停止循环；如果使用continue语句，可以停止当前循环继续进行下一次循环。通过使用 else 语句，当条件不再成立时，可以运行一次代码块。
  
For循环：for 循环用于迭代序列（即列表，元组，字典，集合或字符串）。
这与其他编程语言中的 for 关键字不太相似，而是更像其他面向对象编程语言中的迭代器方法。通过使用 for 循环，我们可以为列表、元组、集合中的每个项目等执行一组语句。它不需要预先设置索引变量，可以对字符串进行遍历。如果使用break语句，可以在循环遍历所有项目之前停止循环；如果使用continue语句，可以停止停止循环的当前迭代，并继续下一个；如需循环一组代码指定的次数，我们可以使用 range() 函数，range(a,b,c) 中a表示从a开始，到b结束不包括b，步长为c；for 循环中的 else 关键字指定循环结束时要执行的代码块；嵌套循环是循环内的循环，外循环每迭代一次，内循环将执行一次；for 语句不能为空，如果写了无内容的for语句，需要使用pass语句来避免错误。
  
If语句(判断语句)：if语句是选取要执行的操作，是python主要的选择工具，代表python程序所拥有的大多数逻辑；if语句是复合语句，同其他复合语句一样，if语句可以包含其他语句，以及逻辑运算符。
 
4.	用dataframe生成数表，进行统计分析(均值,最大最小值,标准差等)
创建Dataframe对象：pd.Dataframe(data,index,colums)，其中data可以为包含列表、字典或者Series的字典；二维数组；一个Series对象；另一个Dataframe对象，index为行索引，colums为列索引。mean()求均值，max()最大值，min()最小值，std()求标准差，mode()求众数，其中需要设置参数axis=1是按行操作，axis=0是按列操作，默认是按列进行操作。
 
5.	异常数据的识别与处理
异常值：指样本中的个别值其数据明显偏离其余的观测值，其也称为离散点。
异常值判别：检验数据是否有录入错误以及含有不合常理的数据。
异常值的判别方法：统计判别法( 原则，肖维勒准则，格拉布斯准则，狄克逊准则，t检验)和箱型图分析法
 
 
 
 
 
 
 
 
异常值处理方法：异常数据可以当做缺失值来处理，处理方法(删除带有缺失值的样本或特征，采用某种方法对缺失值进行填补，插值法，哑变量算法和EM算法，不处理缺失值)
6.	神经网络分类,常用激活函数和损失函数，怎样选取激活函数和损失函数，五种常用优化器的优缺点。
神经网络
①　按拓扑结构进行划分：单层和多层
②　按学习方式进行划分：感知机(有指导)和认知机(无指导)
③　按连接方式进行划分：前馈式神经网络和反馈式神经网络
前馈式神经网络：连接是单向的，上层节点的输出是下层节点的输入
其中包括BP神经网络、卷积神经网络、径向基神经网络、残差神经网络、循环神经网络
反馈式神经网络：除单向连接外，输出节点的输出又作为输入节点的输入
其中包括Hopfield神经网络、Elman神经网络、玻尔兹曼机

激活函数
Sigmod函数
  
Tanh函数
  
ReLU函数
  
ELU函数
  
PReLU函数
  
损失函数
0-1损失
 
平均绝对误差损失（L1损失）
 
均方差损失（L2损失）
 
指数损失
 
Hinge 损失函数
 
交叉熵损失函数
 
SGD（无Momentum）
优点
1、针对大数据集，训练速度很快
2、从训练集样本中随机选取一个batch计算一次梯度，更新一次模型参
3、计算逻辑简单，容易实现
缺点
1、有可能会陷入局部最小值
2、选择合适的learning rate比较困难
3、在所有方向上统一的缩放梯度，不适用于稀疏数据
SGDM（含Momentum）
优点
1、下降初期时，使用上一次参数更新，下降方向一致，乘上较大的β能够进行很好的加速。
2、下降中后期时，在局部最小值来回震荡的时候，梯度g趋近于零，β动量因子使得更新幅度增大，跳出鞍点，加速收敛
缺点
由于t时刻的下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向，因此具有一定的盲目性，相当于小球从山上滚下来时，盲目地沿着坡滚。

Adagrad，在SGD基础上增加二阶动量
优点
1、不同的变量提供不同的学习率。减少摆动，在稀疏数据场景下表现会非常好；
2、允许使用一个更大的学习率lr，从而加快算法的学习速度；这个在后期，会通过分母来整体减小学习率
缺点
因为是不断累积单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即使后续还有数据也无法学到需要的知识；

RMSProp
优点
1、自适应调节学习率
2、对学习率进行了约束，适合处理非平稳目标和RNN。
缺点
RMSprop依然依赖于全局学习率

ADAM
优点
1、Adam 是一种在深度学习模型中用来替代随机梯度下降的优化算法。
2、Adam 结合了 AdaGrad 和 RMSProp 算法最优的性能，它还是能提供解决稀疏梯度和噪声问题的优化方法。
3、Adam 的调参相对简单，默认参数就可以处理绝大部分的问题
缺点
Adam的泛化性并不如SGDM，Adam中的L2正则化项也并不像在SGD中那么有效，SGD虽然训练时间更长，容易陷入鞍点，但是在好的初始化和学习率调度方案的情况下，结果更可靠。
7. 欠拟合,过拟合产生的原因及处理方法
过拟合和欠拟合是导致模型泛化能力不高的两种常见原因，都是模型学习能力与数据复杂度之间失配的结果。欠拟合常常在模型学习能力较弱，而数据复杂度较高的情况出现，此时模型由于学习能力不足，无法学习到数据集中的一般规律，因而导致泛化能力弱。过拟合常常在模型学习能力过强的情况中出现，此时的模型学习能力太强，以至于将训练集单个样本自身的特点都能捕捉到，并将其认为是一般规律，同样这种情况也会导致模型泛化能力下降。过拟合与欠拟合的区别在于，欠拟合在训练集和测试集上的性能都较差，而过拟合往往能较好地学习训练集数据的性质，而在测试集上的性能较差。在神经网络训练的过程中，欠拟合主要表现为输出结果的高偏差，而过拟合主要表现为输出结果的高方差。
欠拟合主要产生的原因：
模型复杂度过低，无法很好的去拟合所有的训练数据，导致训练误差大，特征量过少
解决方法：
1.	增加模型复杂度，尝试使用核SVM、决策树、深度神经网络（DNN）
2.	增加新特征，可以考虑加入特征组合、高次特征，来增大假设空间
3.	添加多项式特征，将线性模型通过添加二次项或者三次项使模型泛化能力更强
4.	减少正则化参数，正则化的目的是用来防止过拟合的，模型出现了欠拟合，则需要减少正则化参数
5.	调整模型的容量，模型的容量是指其拟合各种函数的能力
6.	容量低的模型可能很难拟合训练集；使用集成学习方法
过拟合主要产生的原因：
1. 建模样本选取有误，如样本数量太少，选样方法错误，样本标签错误等，导致选取的样本数据不足以代表预定的分类规则
2. 样本噪音干扰过大，使得机器将部分噪音认为是特征从而扰乱了预设的分类规则
3. 假设的模型无法合理存在，或者说是假设成立的条件实际并不成立
4. 参数太多，模型复杂度过高
5. 对于决策树模型，如果我们对于其生长没有合理的限制，其自由生长有可能使节点只包含单纯的事件数据或非事件数据，使其虽然可以完美拟合训练数据，但是无法适应其他数据集
6. 对于神经网络模型：a)对样本数据可能存在分类决策面不唯一，随着学习的进行,，BP算法使权值可能收敛过于复杂的决策面；b)权值学习迭代次数足够多，拟合了训练数据中的噪声和训练样例中没有代表性的特征
解决方法：
1.增加训练数据可以有限的避免过拟合
2.正则化，L1、L2；如果有正则项，则考虑增大正则项参数
3.交叉验证
4.特征选择，减少特征数或使用较少的特征组合
8. 常用的预测模型(3种),给定一个预测模型并写出具体的求解步骤
GM(1,1)模型是基于随机的原始时间序列，经按时间累加后形成的新的时间序列所呈现的规律用一阶线性微分方程的解来逼近的模型。适用条件：1.数据量不少于4个；2. 原始数据非负、符合指数规律变化且变化不是很快。GM(1,1)相当于用一阶微分方程对一个变量建立模型。
GM(1,1)模型建模过程：
 
适用场合：可以预测农业、经济、环境、教育、医药等问题。

Verhulst模型的基本思想是将离散的随机数列x进行一次累加，生成序列y，然后再对序列y     进行建模计算，得到预测值。
Verhulst模型建模过程：
 
适用场合：常用于人口预测、生物生长、繁殖预测等问题。

灰色波形预测模型：当原始数据频繁波动且摆动幅度较大时，往往难以找到适当的模拟模型，这时可以考虑根据原始数据的波形预测未来行为数据发展变化的波形，这种预测称为波形预测。
灰色波形预测模型建模过程：
 
适用场合：短期股票预测、区域降水量预测等问题。

9. 卷积神经网络的结构及具体步骤
 
输入层：输入层是整个神经网络的输入，在处理图像的卷积神经网络中，它一般代表了一张图片的像素矩阵。图中最左侧的三维矩阵的长和宽代表了图像的大小，而三维矩阵的深度代表了图像的色彩通道。从输入层开始，卷积神经网络通过不同的神经网络结构将上一层的三维矩阵转化为下一层的三维矩阵转化为下一层的三维矩阵，直到最后的全连接层。
卷积层：卷积层是一个卷积神经网络中最重要的部分。和传统全连接层不同，卷积层中的每一个节点的输入只是上一层神经网络中的一小块。卷积层试图将神经网络中的每一个小块进行更加深入的分析从而得到抽象程度更高的特征。一般来说，通过卷积层处理的节点矩阵会变得更深。
池化层：池化层神经网络不会改变三维矩阵的深度，但是它可以缩小矩阵的大小。池化操作可以认为是将一张分辨率较高的图片转化为分辨率较低的图片。通过池化层，可以进一步缩小最后全连接层中节点的个数，从而达到减少整个神经网络中的参数的目的。
全连接层：图中在经过多轮卷积层和池化层处理之后，在卷积神经网络的最后一般会由1到2个全连接层来给出最后的分类结果。经过几轮的卷积层和池化层的处理之后，可以认为图像中的信息已被抽象成了信息含量更高的特征。我们可以将卷积层和池化层看成自动图像特征提取的过程。在特征提取完成之后，仍然需要使用全连接层来完成分类任务。
Softmax层：Softmax层主要用于分类问题。经过Softmax层，可以得到当前样例中属于不同种类的概率分布情况。






