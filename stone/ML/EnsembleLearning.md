# 集成学习

## 参考文献
https://www.cnblogs.com/sddai/p/7647731.html#:~:text=Boosting%E7%AE%97%E6%B3%95.%20Boosting%E7%AE%97%E6%B3%95%E6%8C%87%E5%B0%86%E5%BC%B1%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%BB%84%E5%90%88%E6%88%90%E5%BC%BA%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%8C%E5%AE%83%E7%9A%84%E6%80%9D%E6%83%B3%E8%B5%B7%E6%BA%90%E4%BA%8EValiant%E6%8F%90%E5%87%BA%E7%9A%84PAC%20%28Probably%20Approximately,Correct%29%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E3%80%82.%20%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%EF%BC%9A%E4%B8%8D%E5%90%8C%E7%9A%84%E8%AE%AD%E7%BB%83%E9%9B%86%E6%98%AF%E9%80%9A%E8%BF%87%E8%B0%83%E6%95%B4%E6%AF%8F%E4%B8%AA%E6%A0%B7%E6%9C%AC%E5%AF%B9%E5%BA%94%E7%9A%84%E6%9D%83%E9%87%8D%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%8C%E4%B8%8D%E5%90%8C%E7%9A%84%E6%9D%83%E9%87%8D%E5%AF%B9%E5%BA%94%E4%B8%8D%E5%90%8C%E7%9A%84%E6%A0%B7%E6%9C%AC%E5%88%86%E5%B8%83%EF%BC%8C%E8%80%8C%E8%BF%99%E4%B8%AA%E6%9D%83%E9%87%8D%E4%B8%BA%E5%88%86%E7%B1%BB%E5%99%A8%E4%B8%8D%E6%96%AD%E5%A2%9E%E5%8A%A0%E5%AF%B9%E9%94%99%E5%88%86%E6%A0%B7%E6%9C%AC%E7%9A%84%E9%87%8D%E8%A7%86%E7%A8%8B%E5%BA%A6%E3%80%82.%201.%20%E9%A6%96%E5%85%88%E8%B5%8B%E4%BA%88%E6%AF%8F%E4%B8%AA%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E7%9B%B8%E5%90%8C%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D%EF%BC%8C%E5%9C%A8%E6%AD%A4%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E5%88%86%E5%B8%83%E4%B8%8B%E8%AE%AD%E7%BB%83%E5%87%BA%E4%B8%80%E4%B8%AA%E5%BC%B1%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%9B.%202.

## 所解决的问题
- 单个分类器效果好，为什么不用多个呢：
  - 分类器之间有差异
  - 每个分类器的精度必须大于0.5
- 如何获取多个分类器：
    - 用不同的机器学习算法训练模型
    - 每个模型训练数据的一部分
## boosting

不同的训练集是通过调整每个样本对应的权重实现的，不同的权重对应不同的样本分布，而这个权重为分类器不断增加对错分样本的重视程度。

Adaboost、GBDT、XGBOOST

步骤：



## Bagging（Bootstrap Aggregating）

采用的是随机有放回的选择训练数据然后构造分类器，最后组合。

每次训练数据时只使用训练集中的某个子集作为当前训练集（有放回随机抽样），每一个训练样本在某个训练集中可以多次或不出现，经过T次训练后，可得到T个不同的分类器。对一个测试样例进行分类时，分别调用这T个分类器，得到T个分类结果。最后把这T个分类结果中出现次数多的类赋予测试样例。这种抽样的方法叫做bootstrap，就是利用有限的样本资料经由多次重复抽样，重新建立起足以代表原始样本分布之新样本。

随机森林



