<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.1.23" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>SVM: 技术 — 优化理论与算法基础</title>
</head>
<body class="tc-body">

<section class="tc-story-river tc-static-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists  tc-tagged-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95" data-tags="机器学习方法" data-tiddler-title="SVM"><div class="tc-tiddler-title"><div class="tc-titlebar"><span class="tc-tiddler-controls"><span class=" tc-reveal"><button aria-label="更多" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="更多动作"></button><div class=" tc-reveal" hidden="true"></div></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="编辑" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="编辑此条目"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal"><button aria-label="关闭" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="关闭此条目"></button></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span><span class=" tc-reveal" hidden="true"></span></span><span><h2 class="tc-title">SVM</h2></span></div><div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div></div><div class=" tc-reveal" hidden="true"></div>
<div class=" tc-reveal"><div class="tc-subtitle"><a class="tc-tiddlylink tc-tiddlylink-missing" href=".html"></a>2021年02月27日 07:49</div></div><div class=" tc-reveal">
<div class="tc-tags-wrapper"><span class="tc-tag-list-item">
<span class="tc-tag-label tc-btn-invisible" draggable="true" style="background-color:;
fill:#333333;
color:#333333;">
机器学习方法
</span>
<span class="tc-drop-down tc-reveal" hidden="true"></span></span></div>
</div>

<div class="tc-tiddler-body tc-reveal"><dl><dt>SVM多分类</dt></dl><p>SVM多分类原理：对于样本中的每两个类别之间都训练一个SVM二分类器。对于k个类别， 共可训练出k(k-1)/2个SVM二分类器。在预测时，将测试样例分别输入到k(k-1)/2分类器中。
假设（i,j)表示划分类别i和类别j的SVM分类器
对于每个分类器(i,j)：
若分类结果为+1，则count[i] +=1
若分类结果为-1，则count[j] +=1
最后分类结果取相应类别计数最大的那个类别作为最终分类结果</p><dl><dt>随机森林的训练过程如下：</dt></dl><ol><li>给定训练集S，测试集T，特征维数F。确定参数：使用到的CART的数量t，每棵树的深度d，每个节点使用到的特征数量f，终止条件：节点上最少样本数s，节点上最少的信息增益m;</li></ol><p>对于第1-t棵树，i=1-t：</p><ol><li>从S中有放回的抽取大小和S一样的训练集S(i)，随机的选择作为根节点的样本，从根节点开始训练</li><li>如果当前节点上达到终止条件，则设置当前节点为叶子节点，如果是分类问题，该叶子节点的预测输出为当前节点样本集合中数量最多的那一类c(j)，概率p为c(j)占当前样本集的比例；如果是回归问题，预测输出为当前节点样本集各个样本值的平均值。然后继续训练其他节点。如果当前节点没有达到终止条件，则从F维特征中无放回的随机选取f维特征。利用这f维特征，寻找分类效果最好的一维特征k及其阈值th，当前节点上样本第k维特征小于th的样本被划分到左节点，其余的被划分到右节点。继续训练其他节点。有关分类效果的评判标准在后面会讲。</li><li>重复(2)(3)直到所有节点都训练过了或者被标记为叶子节点。</li><li>重复(2),(3),(4)直到所有CART都被训练过。</li></ol><p>随机森林的预测过程如下：</p><p>对于第1-t棵树，i=1-t：</p><ol><li>从当前树的根节点开始，根据当前节点的阈值th，判断是进入左节点(&lt;th)还是进入右节点(&gt;=th)，直到到达，某个叶子节点，并输出预测值。</li><li>重复执行(1)直到所有t棵树都输出了预测值。如果是分类问题，则输出为所有树中预测概率总和最大的那一个类，即对每个c(j)的p进行累计；如果是回归问题，则输出为所有树的输出的平均值。</li></ol><p>注：有关分类效果的评判标准，因为使用的是CART，因此使用的也是CART的平板标准，和C3.0,C4.5都不相同。</p><p>对于分类问题（将某个样本划分到某一类），也就是离散变量问题，CART使用Gini值作为评判标准。定义为Gini=1-∑(P(i)*P(i)),P(i)为当前节点上数据集中第i类样本的比例。例如：分为2类，当前节点上有100个样本，属于第一类的样本有70个，属于第二类的样本有30个，则Gini=1-0.7×07-0.3×03=0.42，可以看出，类别分布越平均，Gini值越大，类分布越不均匀，Gini值越小，在寻找最佳的分类特征和阈值时。</p><p>1：评判标准为：argmax（Gini-GiniLeft-GiniRight），即寻找最佳的特征f和阈值th，使得当前节点的Gini值减去左子节点的Gini和右子节点的Gini值最大。</p><p>2：对于回归问题，相对更加简单，直接使用argmax(Var-VarLeft-VarRight)作为评判标准，即当前节点训练集的方差Var减去减去左子节点的方差<a class="tc-tiddlylink tc-tiddlylink-missing" href="VarLeft.html">VarLeft</a>和右子节点的方差<a class="tc-tiddlylink tc-tiddlylink-missing" href="VarRight.html">VarRight</a>值最大。</p><p>Random Forest与Bagging的区别在于：Bagging每次生成决策树的时候从全部的属性Attributes里面选择，而Random Forest是随机从全部Attributes的集合里面生成一个大小固定的子集，相对而言需要的计算量更小一些。
          </p></div>

<div class="tc-tiddler-plugin-info tc-reveal"><p>
</p></div>
</div>

</p>
</section>
</body>
</html>

